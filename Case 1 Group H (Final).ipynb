{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd17a52",
   "metadata": {},
   "source": [
    "# Customer Segmentation Case 1 <img align='right' width='120' height='150' src='https://maiseducativa.com/wp-content/uploads/2015/02/Logo_Nova-IMS.jpg'>\n",
    "\n",
    "## <font color='SeaGreen'>__Business Cases with Data Science__</font><br>\n",
    "\n",
    "> Group H composed by:\n",
    ">> __Tomás Moura Vicente, nº20221355__ <p>\n",
    ">> __Lukas Gross, nº20221363__ <p>\n",
    ">> __Karim Miladi, nº20210719__ <p>\n",
    ">> __Tomás Domingos, nº20221643__ <p>\n",
    ">> __Beatriz Carmo, nº20221355__ <p>\n",
    "    \n",
    "***\n",
    "\n",
    "## 📖 Introduction\n",
    "    \n",
    " Finding new customers is vital in every industry. The process of finding new customers begins by learning as much as possible from the existing customers. Understanding current customers allow organizations to identify groups of customers that have different product interests, different market participation, or different response to marketing efforts. \n",
    " Market segmentation, the process of identifying customers’ groups, makes use of geographic, demographic, psychographic, and behavioral characteristics of customers. By understanding the differences between the different segments, organizations can make better strategic choices about opportunities, product definition, positioning, promotions, pricing, and target marketing (1, 2).\n",
    "\n",
    " Hotel H, a hotel located in Lisbon, Portugal, a member of the independent hotel chain C, uses a hospitality standard market segmentation based on the origin of the customer. However, A, the new marketing manager of hotel H, recognized that this type of segmentation, as is today well-known (3, 4), is not useful for the hotel marketing department.\n",
    "    \n",
    "***    \n",
    "### 💾 Key Problems\n",
    "\n",
    "Until 2015 hotel chain C operated 4 hotels, however, with the acquisition of new hotels, the hotel chain board decided to invest more in marketing. However, it was not until 2018 that the hotel chain created a marketing department and hired a new marketing manager, A. A realized that the current customer segmentation was not adequate, as it only reflected one only customer characteristic, its sales origin. It did not reflect geographic characteristics, such as the country of origin, demographic characteristics, such as age, or behavioral characteristics, such as the number of stays.\n",
    "Without proper customer segmentation, it is difficult for A to define a strategy to reach new customers and to continue to captivate the current customers. Taking into consideration the multiple distribution channels that \n",
    "hotels operate nowadays (travel agencies, travel operators, online travel agencies – OTA, brand websites, meta searchers websites, among others). For example, corporate customers tend to make reservations very near the arrival date, book directly with the hotel, and be willing to pay more for a better-equipped room, while a customer on holiday tends to make reservations more distant from the arrival date, book with a travel operator or OTA, and to look for better price opportunities. Therefore, products “creation”, pricing definitions, and other marketing tasks, such as advertising, must take into consideration the targets of its efforts according to the different channels and groups of customers.\n",
    "\n",
    "***    \n",
    "### 💾 Data \n",
    "\n",
    "The data provided gives __access to a small quantity of sociodemographic, health, and behavioral information obtained from the Patients__. Its divided by sets of training and test data. __The training data will be the one used to build the machine learning models__. In this set, there's also the __ground truth associated to each Patient, i.e., if the Patient has the disease or not__.\n",
    "The test set will be used __to see how well the model performs on unseen data__, since it provides no access to the __ground truth__.\n",
    "    \n",
    "There are available 2 sets of csv:\n",
    "\n",
    ">> Case1_HotelCustomerSegmentation.csv <p>\n",
    ">> UNSD — Methodology.csv        \n",
    "    \n",
    "<br>\n",
    "<details open>\n",
    "    <summary> \n",
    "       <font color='SeaGreen' size =3>Click here to SEE/HIDE Feature Description</font>\n",
    "    </summary><br>\n",
    "    \n",
    "### Customer Details <p>\n",
    "- __Customer ID__ - A unique identifier assigned to each customer <p>\n",
    "- __Nationality__ - The nationality of the customer in ISO 3166-1 (Alpha 3) format <p>\n",
    "- __Age__ - The age of the customer <p>\n",
    "- __DaysSinceCreation__ - The number of elapsed days since the customer was created <p>\n",
    "- __NameHash__ - A hash of the customer's name <p>\n",
    "- __DocIDHash__ - A hash of the customer's personal document identification number (usually a passport or ID card) <p>\n",
    "    \n",
    "### Revenue and Booking Information <p>\n",
    "- __AverageLeadTime__ - The average number of days before the arrival date the customer makes bookings <p>\n",
    "- __LodgingRevenue__ - The total amount of lodging revenue paid by the customer so far <p>\n",
    "- __OtherRevenue__ - The total amount of other revenue (e.g., food & beverage, spa, etc.) paid by the customer so far <p>\n",
    "- __BookingsCanceled__ - The number of bookings the customer made but subsequently canceled <p>\n",
    "- __BookingNoShowed__ - The number of bookings the customer made but subsequently made a \"no-show\" <p>\n",
    "- __BookingsCheckedin__ - The number of bookings the customer made, which actually ended up staying <p>\n",
    "\n",
    "    \n",
    "### Room and Person Nights <p>\n",
    "- __PersonNights__ - The total person/nights the customer has stayed at the hotel so far. Persons/Nights are the sum of Adults and Children in each booking, multiplied by the number of Nights (Length-of-stay) of the booking <p>\n",
    "- __RoomNights__ - The total room/nights the customer has stayed at the hotel so far. Room/Nights are the multiplication of the number of rooms of each booking by the number of Nights (Length-of-stay) of the booking <p>\n",
    "\n",
    "\n",
    "### Preferences and Requests <p>\n",
    "- __DistributionChannel__ - The distribution channel normally used by the customer to make bookings at the hotel <p>\n",
    "- __MarketSegment__ - The current market segment of the customer <p>\n",
    "- __SRHighFloor__ - Indication if the customer usually asks for a room in a higher floor (0: No, 1: Yes) <p>\n",
    "- __SRLowFloor__ - Indication if the customer usually asks for a room in a lower floor (0: No, 1: Yes) <p>\n",
    "- __SRAccessibleRoom__ - Indication if the customer usually asks for an accessible room (0: No, 1: Yes) <p>\n",
    "- __SRMediumFloor__ - Indication if the customer usually asks for a room in a middle floor (0: No, 1: Yes) <p>\n",
    "- __SRBathtub__ - Indication if the customer usually asks for a room with a bathtub (0: No, 1: Yes) <p>\n",
    "- __SRShower__ - Indication if the customer usually asks for a room with a shower (0: No, 1: Yes) <p>\n",
    "- __SRCrib__ - Indication if the customer usually asks for a crib (0: No, 1: Yes) <p>\n",
    "- __SRKingSizeBed__ - Indication if the customer usually asks for a room with a king size bed (0: No, 1: Yes) <p>\n",
    "- __SRTwinBed__ - Indication if the customer usually asks for a room with a twin bed (0: No, 1: Yes) <p>\n",
    "- __SRNearElevator__ - Indication if the customer usually asks for a room near the elevator (0: No, 1: Yes) <p>\n",
    "- __SRAwayFromElevator__ - Indication if the customer usually asks for a room away from the elevator (0: No, 1: Yes) <p>\n",
    "- __SRNoAlcoholInMiniBar__ - Indication if the customer usually asks for a room with no alcohol in the minibar (0: No, 1: Yes) <p>\n",
    "- __SRQuietRoom__ - Indication if the customer usually asks for a room away <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f6749",
   "metadata": {},
   "source": [
    "***\n",
    "### 📋 Index\n",
    "\n",
    "* [1. Business Understanding](#1)\n",
    "* [2. Data Understanding](#2)\n",
    "    * [2.1. Loading and Collecting Data](#2.1)\n",
    "    * [2.2. Describing Data](#2.2)\n",
    "    * [2.3. Exploring Data](#2.3)\n",
    "    * [2.4. Data Quality Check](#2.4)\n",
    "    * [2.5. Data Relationships](#2.5)\n",
    "    \n",
    "* [3. Data Preperation](#3)\n",
    "    * [3.1. Data Cleaning](#3.1)\n",
    "    * [3.2. Feature Engineering](#3.2)\n",
    "    * [3.3. Data Transformation](#3.3)\n",
    "* [4. Modeling](#4)\n",
    "    * [4.1. Funnel Data](#4.1)\n",
    "        * [4.1.1. Analyzing Data with PCA](#4.1.1)\n",
    "        * [4.1.2. Applying K-Means](#4.1.2)\n",
    "    * [4.2. Geo Data](#4.2)\n",
    "        * [4.2.1. Analyzing Data with PCA](#4.2.1)\n",
    "        * [4.2.2. Applying K-Means](#4.2.2)     \n",
    "* [5. Cluster Merging](#5)\n",
    "* [6. Evaluation](#6)\n",
    "* [7. Cluster Profiling](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492b49c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cc46b",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install graphviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from yellowbrick.cluster import InterclusterDistance\n",
    "from matplotlib import ticker \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#others\n",
    "%autosave 90\n",
    "pd.set_option('display.max_columns', 30)\n",
    "RS = 42 # random state\n",
    "np.random.seed(RS)\n",
    "# ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#colour palette\n",
    "my_colors=[\"#824D99\", \"#4E78C4\", \"#57A2AC\", \"#7EB875\", \"#B3AF38\", \"#A75051\", '#b27d58']\n",
    "\n",
    "#visualization palette\n",
    "subPlots_Title_fontSize = 12\n",
    "subPlots_xAxis_fontSize = 10\n",
    "subPlots_yAxis_fontSize = 10\n",
    "subPlots_label_fontSize = 10\n",
    "heatmaps_text_fontSize = 8\n",
    "\n",
    "plots_Title_fontSize = 14\n",
    "plots_Title_textColour = 'black'\n",
    "\n",
    "plots_Legend_fontSize = 12\n",
    "plots_Legend_textColour = 'black'\n",
    "\n",
    "plots_barTexts_fontSize = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c94052",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4cde21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Describe(df):\n",
    "\n",
    "    '''\n",
    "    Describes data about the Dataset\n",
    "    '''\n",
    "    unique_vals = pd.DataFrame(\n",
    "        [(col, df[col].nunique()) for col in df.columns], \n",
    "        columns =['Feature', 'N_unique_vals']\n",
    "    )\n",
    "\n",
    "    unique_vals.set_index('Feature', inplace = True)\n",
    "    unique_vals = pd.concat([unique_vals, pd.DataFrame(df.dtypes).rename(columns = {0:'d_type'})], axis = 1)\n",
    "    unique_vals['N_nulls'] = df.isna().sum().values\n",
    "\n",
    "    obj_cols = unique_vals[unique_vals['d_type'] == 'object'].index\n",
    "\n",
    "    for col in obj_cols:\n",
    "        unique_vals.loc[col, 'N_nulls'] += len(df[(df[col] == '') | (df[col] == ' ')])\n",
    "    \n",
    "    print(f'Data size: {df.shape[0]} x {df.shape[1]}')\n",
    "    return unique_vals[['N_unique_vals', 'N_nulls', 'd_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_reporter(data, threshold=None):\n",
    "    \n",
    "    '''\n",
    "    Returns pandas dataframe with feature's missing values count in absolute \n",
    "    and relative frequency\n",
    "    after a threshold is parsed (max % of column missing values), so it'll \n",
    "    tell if the features are \n",
    "    above or bellow the desired missing threshold limit (True for above or \n",
    "    False for bellow).\n",
    "    \n",
    "    If the threshold is not provided, it defaults to None.\n",
    "     Args:\n",
    "        data - input for data\n",
    "        threshold - input for threshold variable from 0 to 1\n",
    "        \n",
    "    '''\n",
    "    na_count = data.isna().sum() \n",
    "    na_count = na_count[na_count > 0]\n",
    "    na_abs_frq = na_count.values\n",
    "    na_rel_frq = round(na_count/len(data),2)\n",
    "    missings = pd.DataFrame({'Feature': na_count.index, 'Nº of missings': \n",
    "    na_abs_frq, '% of missings': na_rel_frq})\n",
    "    missings = missings.sort_values(by = 'Nº of missings', ascending = False)\n",
    "    \n",
    "    if threshold:\n",
    "        missings['Above threshold'] = [True if x > threshold else False for x \n",
    "        in missings['% of missings']]\n",
    "        \n",
    "    return missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_summary_stats(data):\n",
    "\n",
    "    # Select only numerical columns\n",
    "    num_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    # Calculate summary statistics for numerical columns\n",
    "    summary_stats = data[num_cols].describe().T.reset_index()\n",
    "\n",
    "    # Rename columns for readability\n",
    "    summary_stats.rename(columns={'index': 'column', 'count': 'non_missing', \n",
    "                                  '50%': 'median', 'std': 'stdev'}, \n",
    "                                  inplace=True)\n",
    "\n",
    "    # Calculate coefficient of variation (CV)\n",
    "    summary_stats['CV'] = summary_stats['stdev'] / summary_stats['mean']\n",
    "\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_summary_stats(data):\n",
    "\n",
    "    # Select only categorical columns\n",
    "    cat_cols = data.select_dtypes(include=['object',  \n",
    "    'category']).columns.tolist()\n",
    "\n",
    "    # Calculate frequency counts for each categorical column\n",
    "    freq_counts = pd.concat([data[col].value_counts().reset_index() for col \n",
    "    in cat_cols])\n",
    "\n",
    "    # Rename columns for readability\n",
    "    freq_counts.rename(columns={'index': 'category', cat_cols[0]: 'count'}, \n",
    "    inplace=True)\n",
    "\n",
    "    # Add a column for the proportion of each category\n",
    "    freq_counts['proportion'] = freq_counts['count'] / freq_counts['count'].sum()\n",
    "\n",
    "    return freq_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_check(data): \n",
    "    '''\n",
    "    Displays distribution of numerical data\n",
    "    \n",
    "    Args:\n",
    "        data (pandas dataframe) - dataset to check\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.pyplot subplots\n",
    "    '''\n",
    "    # Getting numerical features, excluding binaries\n",
    "    nums = [col for col in data.select_dtypes(include=np.number) \\\n",
    "            if len(data[col].unique()) > 2]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(nrows = 4,\n",
    "                             ncols = ceil(len(nums)/4),\n",
    "                             figsize = (15,17), \n",
    "                             constrained_layout = True\n",
    "                            )\n",
    "\n",
    "    # Plot data\n",
    "    # Iterating across axes objects and associating each histogram\n",
    "    for ax, col in zip(axes.flatten(), nums):\n",
    "        sns.distplot(data[col], hist=True, color = 'lightsteelblue', ax=ax)\n",
    "        ax.set_title(col.replace('_', ' '), y = 1, fontsize=14)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "    # Layout\n",
    "    plt.suptitle('Numerical Features Distribution', fontsize = 18, fontweight='bold')\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=.935, wspace=None, hspace=None) \n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(df):\n",
    "    '''\n",
    "    Returns main information about the Dataset\n",
    "    '''\n",
    "    unique_vals = pd.DataFrame(\n",
    "        [(col, df[col].nunique()) for col in df.columns], \n",
    "        columns =['Feature', 'N_unique_vals']\n",
    "    )\n",
    "\n",
    "    unique_vals.set_index('Feature', inplace = True)\n",
    "    unique_vals = pd.concat([unique_vals, pd.DataFrame(df.dtypes).rename(columns = {0:'d_type'})], axis = 1)\n",
    "    unique_vals['N_nulls'] = df.isna().sum().values\n",
    "\n",
    "    obj_cols = unique_vals[unique_vals['d_type'] == 'object'].index\n",
    "\n",
    "    for col in obj_cols:\n",
    "        unique_vals.loc[col, 'N_nulls'] += len(df[(df[col] == '') | (df[col] == ' ')])\n",
    "    \n",
    "    print(f'Data size: {df.shape[0]} x {df.shape[1]}')\n",
    "    return unique_vals[['N_unique_vals', 'N_nulls', 'd_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_boxplot(df, general_info, name):\n",
    "    '''\n",
    "    Shows boxplots for all continuous variables\n",
    "    '''\n",
    "    exception_list = ['ID']\n",
    "    numerics_vals = general_info[(general_info['N_unique_vals'] > 5) & (general_info['d_type'] != 'object')].index\n",
    "    fig = make_subplots(rows=1, cols=len(numerics_vals))\n",
    "    for i, var in enumerate(numerics_vals):\n",
    "        if var not in exception_list:\n",
    "            fig.add_trace(\n",
    "                go.Box(y=df[var], name=var, marker_color = 'rgb(87, 106, 158)', line = {\"color\":\"black\"}),\n",
    "                row=1, \n",
    "                col= i+1)\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1200,\n",
    "        height=600)\n",
    "    fig.update_traces(boxpoints='outliers', jitter=.3, fillcolor = 'rgb(87, 106, 158)')\n",
    "    fig.update_layout(title=\"Boxplots\", showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlations(data, method):\n",
    "\n",
    "    corrs = data.corr(method = method).abs()\n",
    "    corr_df = corrs.unstack().sort_values(ascending = False)\n",
    "    corr_df = corr_df.reset_index()\n",
    "    corr_df = corr_df[corr_df['level_0'] != \n",
    "    corr_df['level_1']].reset_index(drop = True)\n",
    "    corr_df = corr_df.iloc[::2, :]\n",
    "    corr_df.columns = ['Var_1', 'Var_2', 'Corr_score']\n",
    "    return corr_df\n",
    "\n",
    "def corr_plot(data, method):\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    heatmap = sns.heatmap(data.corr(method = method), vmin=-1, vmax=1, \n",
    "    annot=True, cmap=\"Blues\")\n",
    "    heatmap.set_title(f'{method.capitalize()} Correlation Heatmap', fontdict=\n",
    "    {'fontsize':12}, pad=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e96de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_fixer(data):\n",
    "        data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        columns_to_fill_0 = [_ for _ in data.columns if 'Prem' in _]\n",
    "        cols_2_crop = []\n",
    "        for col in columns_to_fill_0:\n",
    "            data[col] = data[col].fillna(0)\n",
    "        for col in data.columns:\n",
    "            try:\n",
    "                data[col] = data[col].fillna(data[col].median())\n",
    "            except:\n",
    "                cols_2_crop.append(col)\n",
    "        data.dropna(subset = cols_2_crop, inplace = True)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ad7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_remaining_data_IQR(data, criterion):\n",
    "    '''\n",
    "    Computes the percentage of remaining data after removing outliers using IQR method with specified criterion.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.core.frame.DataFrame): input data\n",
    "        criterion (float): criterion for outlier removal (IQR multiplier)\n",
    "        \n",
    "    Returns:\n",
    "        None: prints percentage of remaining data after outlier removal using IQR method with specified criterion\n",
    "    '''\n",
    "    Q1 = data.quantile(.25) # value of first quartile\n",
    "    Q3 = data.quantile(.75) # value of third quartile\n",
    "    IQR = Q3 - Q1 # interquartile range\n",
    "    lower_lim = Q1 - criterion * IQR # setting min limit\n",
    "    upper_lim = Q3 + criterion * IQR # setting max limit\n",
    "\n",
    "    filters = []\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        llim = lower_lim[col]\n",
    "        ulim = upper_lim[col]\n",
    "        filters.append(data[col].between(llim, ulim, inclusive='both'))\n",
    "\n",
    "    data_no_outliers = data[np.all(filters, axis=0)]\n",
    "    percentage_remaining = round(data_no_outliers.shape[0] / data.shape[0] * 100)\n",
    "\n",
    "    print(percentage_remaining, '%', 'data kept using IQR')\n",
    "\n",
    "    \n",
    "def percentage_remaining_data_Zscore(data, criterion):\n",
    "    '''\n",
    "    Computes the percentage of remaining data after removing outliers using Z-score method with specified criterion.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.core.frame.DataFrame): input data\n",
    "        criterion (float): criterion for outlier removal (number of standard deviations from the mean)\n",
    "        \n",
    "    Returns:\n",
    "        None: prints percentage of remaining data after outlier removal using Z-score method with specified criterion\n",
    "    '''\n",
    "    mean = data.mean() # mean of each column\n",
    "    std = data.std() # standard deviation of each column\n",
    "    lower_lim = mean - criterion * std # setting min limit\n",
    "    upper_lim = mean + criterion * std # setting max limit\n",
    "\n",
    "    filters = []\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        llim = lower_lim[col]\n",
    "        ulim = upper_lim[col]\n",
    "        filters.append(data[col].between(llim, ulim, inclusive='both'))\n",
    "\n",
    "    data_no_outliers = data[np.all(filters, axis=0)]\n",
    "    percentage_remaining = round(data_no_outliers.shape[0] / data.shape[0] * 100)\n",
    "\n",
    "    print(percentage_remaining, '%', 'data kept using Z-score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=figsize, squeeze=False)\n",
    "    for label, titl in zip( label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=[\"#824D99\", \"#4E78C4\", \"#57A2AC\", \"#7EB875\", \"#B3AF38\", \"#A75051\", '#b27d58'],\n",
    "                                         ax=ax[0,0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1,0],  palette=[\"#824D99\", \"#4E78C4\", \"#57A2AC\", \"#7EB875\", \"#B3AF38\", \"#A75051\", '#b27d58'])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0,0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0,0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=18, fontweight = 'heavy') \n",
    "        ax[0,0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0,0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0,0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=18)\n",
    "        ax[0,0].set_xticklabels(ax[0,0].get_xticklabels(), rotation=-20)\n",
    "        ax[1,0].set_xticklabels(cluster_labels)\n",
    "        ax[1,0].set_xlabel(\"\")\n",
    "        ax[1,0].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1,0].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=18)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c35be",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\">\n",
    "\n",
    "## <font color='SeaGreen'> 1. Business Understanding </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ade86",
   "metadata": {},
   "source": [
    " The business problem is that Hotel H, a member of hotel chain C located in Lisbon, Portugal, has an inadequate customer segmentation strategy based only on sales origin. The new marketing manager, A, recognizes that this strategy does not reflect important customer characteristics, such as geography, demographics, and behavior, making it difficult to define a strategy to reach new customers and retain current ones. With multiple distribution channels to consider, it is essential to develop a new customer segmentation strategy that takes into account the targets of marketing efforts according to different channels and groups of customers. <p>\n",
    "   Concluding this, we will compare the old segmentation to the new one, and advise some marketing ideas considering all the distribution channels available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ead5fd",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\">\n",
    "    \n",
    "## <font color='SeaGreen'> 2. Data Understanding </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c37384",
   "metadata": {},
   "source": [
    "The Data Description phase aims to preliminarily understand the structure of the data, this process will allow the familiarization with the data, providing the first impressions of patterns, relations, errors and associated changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa585a",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.1\">\n",
    "\n",
    "### 2.1 Loading and Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89059d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/toomingos/Hotel-Case-Study/main/Case1_HotelCustomerSegmentation.csv'\n",
    "df = pd.read_csv(url, delimiter=';')\n",
    "\n",
    "# Dataset is now stored in a Pandas Dataframe\n",
    "\n",
    "url2 = 'https://raw.githubusercontent.com/toomingos/Hotel-Case-Study/main/UNSD%20%E2%80%94%20Methodology.csv'\n",
    "Regions = pd.read_csv(url2, delimiter=';')\n",
    "Regions = Regions[['ISO-alpha3 Code','Sub-region Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c41ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe3dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c242e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9cbfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Looking at the DataFrame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4707bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a48d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regions=Regions[['ISO-alpha3 Code','Sub-region Name']]\n",
    "Regions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('MarketSegment').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb564a18",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "    \n",
    "> 1 - The original hotel dataset has 28 variables.<p>\n",
    "> 3 - There are 111 duplicated rows. <p>\n",
    "> 2 - We imported a additional Dataset called 'Regions' in order to be able to have more information about the origin of the customer. Of this Dataset we only keep the ISO-alpha 3 code column, which corresponds to the 'Nationality' column of the hotel dataset, aswell as the 'Sub-region Name' column.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94fd209",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.2\">\n",
    "\n",
    "### 2.2 Describing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631f595",
   "metadata": {},
   "source": [
    "Analyzing if there are any inconsistencies in form of missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing empty values with nan\n",
    "df.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describing data\n",
    "Describe(df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing strange values\n",
    "strange_values = ['!', '$','%','?','*','+','_','@','€',' ','{']\n",
    "df.replace(strange_values, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking missing values\n",
    "missing_value_reporter(df, threshold=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed71513",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "    \n",
    "> 1 - Two variables have missing values: <p>\n",
    "    - Age (4172 missing values) <p>\n",
    "    - DocIDHash (1001 missing values) <p>\n",
    "\n",
    "> 2 - Replacing strange values did not add new missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34e9da",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.3\">\n",
    "\n",
    "### 2.3 Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0464c",
   "metadata": {},
   "source": [
    "Analyzing the descriptive statistics of the dataset such as mean, median, mode, standard deviation, aswell as the underlying datatypes of the variables and the associated distributions of values in order to gain insights into the nature of the data, identify errors and anomalies, and choose appropriate analytical techniques for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting Features per type\n",
    "qualitative = [col for col in df.select_dtypes('object').columns]\n",
    "quantitative = ['Age','DaysSinceCreation','AverageLeadTime','LodgingRevenue','OtherRevenue','BookingsCanceled','BookingsNoShowed','BookingsCheckedIn','PersonsNights','RoomNights']\n",
    "binary = [df.columns.str.startswith('SR')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b16ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_summary_stats(df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58282fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summary_stats(df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440609a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_check(df[quantitative])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc21fdd",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "    \n",
    "Splitting the variables in 5 qualitative (categorical), 10 quantitative (numerical) features and 13 binary features in order to be able to manipulate the data in later steps independently of each other. This is necessary because different preceprocessing steps need to be performed for different data types before they can be used by clustering algorithms.<p>\n",
    "\n",
    "\n",
    "__Distribution Analysis of the Quantitative Features:__\n",
    "\n",
    "> 1 - __Age:__ Most people are around 50 years old. <p>\n",
    "> 2 - __DaysSinceCreation:__ There are four spikes, where more customers werre added to the system, compared to the immediately adjacent points in time, with the most customers around 500 days. <p>\n",
    "> 3 - __AveragedLeadTime:__ Most customers have a low lead time. The amount of customers gradually decreases with increasing lead time.  <p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378316f4",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.4\">\n",
    "\n",
    "### 2.4 Data Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75114bba",
   "metadata": {},
   "source": [
    "Analyzing the quality of the data with boxplots. Boxplots can help identify outliers, which are data points that fall significantly outside the range of the majority of the data. Outliers can have a significant impact on statistical analysis, including clustering, and can skew results. Boxplots represent outliers as individual points beyond the whiskers of the boxplot. By examining the boxplot for outliers, we can identify whether they are genuine data points or errors that need to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_info = info(df)\n",
    "general_info\n",
    "global_boxplot(df, general_info, 'Exploratory_boxplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536e391",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "    \n",
    "- 9 quantitative variables contain outliers. <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23a85c",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2.5\">\n",
    "\n",
    "### 2.5 Data Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ddf185",
   "metadata": {},
   "source": [
    "Analyzing the relationships in form of correlations between the different variables. This correlation analysis enables us to identify patterns and dependencies between different variables, which is crucial for gaining insights and making informed decisions about the potential removal of varaiables, that do not add value to the final cluster solution in order to reduce the dimensionality, aswell as the redundancy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlations(df[quantitative], 'spearman')\n",
    "corr_plot(df[quantitative], 'spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2f7d5",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "\n",
    " - There are multiple variables with high correlations.\n",
    "\n",
    "__Variables with high correlations:__ \n",
    "    \n",
    "> 1 - 'AverageLeadTime' <p>\n",
    "> 2 - 'OtherRevenue' <p>\n",
    "> 3 - 'BookingsCheckedIn' <p>\n",
    "> 4 - 'PersonsNights' <p>\n",
    "> 5 - 'RoomNights' <p>\n",
    "> 6 - 'RoomNights' <p>\n",
    "\n",
    "Variables with a correlation of 0.7 or more are considered highly correlated and therefore can be considered to be removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90488e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a class=\"anchor\" id=\"3\">\n",
    "\n",
    "## <font color='SeaGreen'> 3. Data Preparation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e65016",
   "metadata": {},
   "source": [
    "Preparing the data by actively manipulating data and variables. This process is important because it helps us to reduce the noise and improve the signal-to-noise ratio of our data, which can improve the accuracy and reliability of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2171f3f",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.1\">\n",
    "\n",
    "### 3.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e834b95",
   "metadata": {},
   "source": [
    "Cleaning the data by removing the outliers. Outliers are observations in a dataset that deviate significantly from other observations, and they can have a disproportionate impact on the quality of clusering solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Missing values once its only in two features DocIDHash and Age\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df_copy=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfiqr15=df\n",
    "dfiqr3=df\n",
    "dfz=df\n",
    "#percentage_remaining_data_IQR(dfiqr15, 1.5) #moderated outliers\n",
    "#percentage_remaining_data_IQR(dfiqr3, 3) #severe outliers\n",
    "#percentage_remaining_data_Zscore(dfz, 3) #z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers with\n",
    "data = df[(df['Age'] >= 18) & (df['Age'] <= 95) & \n",
    "          (df['AverageLeadTime'] < 550) &\n",
    "          (df['LodgingRevenue'] < 4000) &\n",
    "          (df['OtherRevenue'] < 3400) &\n",
    "          (df['BookingsCanceled'] < 6) &\n",
    "          (df['BookingsCheckedIn'] < 25) &\n",
    "          (df['PersonsNights'] < 50) &\n",
    "          (df['RoomNights'] < 40)]\n",
    "\n",
    "    \n",
    "    # Print the percentage of customers remaining\n",
    "print('After excluding the outliers manually,',\n",
    "          f'the dataset will remain with \\033[1m{np.round((len(data)/len(df))*100, 3)}',\n",
    "          '%\\033[0m of its original Customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d036d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#costumers that have a loyalty program\n",
    "mask = (data['BookingsCheckedIn'] == 0) & (data['BookingsCanceled'] == 0) & (data['BookingsNoShowed'] == 0)\n",
    "Loyalty_costumers = data[mask]\n",
    "Loyalty_costumers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(x=Loyalty_costumers['DaysSinceCreation'], name='Days Since Creation'))\n",
    "fig.update_layout(title='How long is the costumer in our loyalty system')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a44d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_BeforeEng = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c0e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_info = info(data)\n",
    "general_info\n",
    "global_boxplot(data, general_info, 'Cleaned_boxplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e12a99",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "> 1 - Dropping missing values of the variables DocIDHash and Age, aswell as dropping duplicates.\n",
    "\n",
    "> 2 - The outlier removal with the IQR methods ( 1.5 and 3 ) aswell as the zscore method did remove large parts of the data. Due to this fact the outlier barriers were chosen manually with logical values from the boxplot analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a686444",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.2\">\n",
    "\n",
    "### 3.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83114ab8",
   "metadata": {},
   "source": [
    "Creating of new features with feature engineering for our clustering solution to improve the ability of the clustering algorithm to identify meaningful patterns and clusters in the data. Feature engineering is an important step in the clustering process as it can improve the quality and meaningfulness of the resulting clusters by providing the clustering algorithm with a more informative and representative feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative = [col for col in df.select_dtypes('object').columns]\n",
    "quantitative = ['Age', 'DaysSinceCreation', 'AverageLeadTime',\n",
    "       'Total_Revenue', 'Booking_Success_Rate', 'Preference_Count']\n",
    "binary = [df.columns.str.startswith('SR')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_Revenue'] = data['LodgingRevenue'] + data['OtherRevenue']\n",
    "#Total Revenue: The sum of lodging revenue and other revenue.\n",
    "data['Service_share'] = np.where(data['Total_Revenue'] == 0, 0, data['OtherRevenue']/data['Total_Revenue'])\n",
    "#Premium_Clien: How much the client spends on services compared to the boobking itself\n",
    "data['Booking_Success_Rate'] = data['BookingsCheckedIn'] / (data['BookingsCheckedIn'] + data['BookingsCanceled'] + data['BookingsNoShowed'])\n",
    "data['Booking_Success_Rate'].fillna(value=0, inplace=True)\n",
    "#Booking Success Rate: The percentage of bookings that were checked in out of all bookings made by the customer.\n",
    "data['Preference_Count'] = data['SRHighFloor'] + data['SRLowFloor'] + data['SRAccessibleRoom'] + data['SRMediumFloor'] + data['SRBathtub'] + data['SRShower'] + data['SRCrib'] + data['SRKingSizeBed'] + data['SRTwinBed'] + data['SRNearElevator'] + data['SRAwayFromElevator'] + data['SRNoAlcoholInMiniBar'] + data['SRQuietRoom']\n",
    "#Room Preference Score: A score based on the customer's room preferences, where each preference has a weight and is summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Zombie costumers \n",
    "\n",
    "# boolean mask to filter out rows\n",
    "mask = ((data['BookingsCheckedIn'] == 0) & \n",
    "        (data['BookingsCanceled'] == 0) & \n",
    "        (data['BookingsNoShowed'] == 0) &\n",
    "        (data['DaysSinceCreation'] > 180) & \n",
    "        (data['Total_Revenue'] == 0))\n",
    "\n",
    "# drop rows that satisfy the mask\n",
    "data = data[~mask]\n",
    "\n",
    "# calculate percentage of data retained\n",
    "percent_retained = len(data) / len(data_BeforeEng) * 100\n",
    "\n",
    "# print percentage retained\n",
    "print(f\"Percentage of data retained: {percent_retained:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative_vars = data.select_dtypes(include=['object', 'category'])\n",
    "print(\"Qualitative Variables:\\n\", qualitative_vars.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66ad51-ad78-49f5-9fa2-4c3be31802eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37253599-15c8-4df9-97b6-ef815736f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qual=data[['Nationality', 'NameHash', 'DocIDHash', 'DistributionChannel', 'MarketSegment']]\n",
    "data_qual.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93094b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ISO_counts = pd.DataFrame(data['Nationality'].value_counts())\n",
    "# reset the index and rename columns\n",
    "data_ISO_counts = data_ISO_counts.reset_index()\n",
    "data_ISO_counts.columns = ['Nationality', 'Count']\n",
    "data_ISO_counts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aabad5-dceb-454d-8ac3-79c7f4564170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add region column by ISO\n",
    "data = pd.merge(data, Regions, left_on='Nationality', right_on='ISO-alpha3 Code')\n",
    "data.drop(['ISO-alpha3 Code'], axis=1, inplace=True)\n",
    "data = data.rename(columns={'Sub-region Name': 'Region'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a20a5-397e-4ac9-8b43-f2b600bf118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Region']. unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a1d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data['Nationality'] == 'PRT', 'Region'] = 'Portugal'\n",
    "data.loc[data['Nationality'] == 'FRA', 'Region'] = 'FRA/GBR'\n",
    "data.loc[data['Nationality'] == 'DEU', 'Region'] = 'Germany'\n",
    "data.loc[data['Nationality'] == 'GBR', 'Region'] = 'FRA/GBR'\n",
    "\n",
    "\n",
    "\n",
    "#Reducing division of Regions by relevancy\n",
    "data['Region'] = data['Region'].replace(['Polynesia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Polynesia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Latin America and the Caribbean'], 'South/North America')\n",
    "data['Region'] = data['Region'].replace(['Northern America'], 'South/North America')\n",
    "data['Region'] = data['Region'].replace(['Micronesia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Melanesia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Western Asia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Central Asia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Southern Asia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['South-eastern Asia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Eastern Asia'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Sub-Saharan Africa'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Northern Africa'], 'Africa/Asia/Oceania')\n",
    "data['Region'] = data['Region'].replace(['Australia and New Zealand'], 'Africa/Asia/Oceania')\n",
    "\n",
    "data_Region_counts = pd.DataFrame(data['Region'].value_counts())\n",
    "# reset the index and rename columns\n",
    "data_Region_counts = data_Region_counts.reset_index()\n",
    "data_Region_counts.columns = ['Region', 'Count']\n",
    "data_Region_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Region'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_reporter(data, threshold=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd7b1e-0fcf-4bdb-acaa-853a24f177df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Nationality','NameHash','DocIDHash'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d45d2",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "> 1 - Creating new variables: <p>\n",
    "     - 'Total_Revenue' ->  the sum of the variables 'LodgingRevenue and 'OtherRevenue'<p>\n",
    "     - 'Service_Share' -> How much the client spends on services compared to the booking itself. <p>\n",
    "     - 'Booking_Success_Rate' -> The percentage of bookings that were checked in out of all bookings made by the customer. <p>\n",
    "     - 'Preference_Count' ->  A score based on the customer's room preferences, where each preference has a weight and is summed up.<p>\n",
    "\n",
    "> 2 - Removal of customers, with no entries in the variables 'BookingsCheckedIn', 'BookingsCanceled', 'BookingsNoShowed' and 'Total_Revenue', that are in the system longer than half a year (180 days). Due to the fact that we classify them as 'Zombie Customers', that are in the system but have no contributions.\n",
    "\n",
    "> 3 - Counting and ordering the most common nationalities.\n",
    "\n",
    "> 4 - Aggregating regions with low number of entries in order to increase their relevancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241d2db",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3.3\">\n",
    "\n",
    "### 3.3 Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9739b13",
   "metadata": {},
   "source": [
    "Transforming the data in form of encoding and scaling. These are critical techniques used to transform the data into a format that is suitable for clustering algorithms. Encoding helps us to transform categorical data into numerical data, while scaling helps us to standardize the range of values across different features in the dataset. By using these techniques, we can improve the accuracy and effectiveness of the clustering algorithm and ensure that each feature contributes equally to the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DistributionChannel', 'Region','MarketSegment']\n",
    "ce_one_hot = ce.OneHotEncoder(cols = cols, use_cat_names=True)\n",
    "data = ce_one_hot.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bb1a8-1886-42d0-b20a-b668e3118c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2ff5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_beforeNorm = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af457f2",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Turning the two categorical variables 'DistributionChannel' and 'Region' into numerical variables with the OneHotEncoder, in order to be able to implement them into the clustering solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4f57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_summary_stats(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec6e85",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Scaling of the dataset with the MinMaxScaler. This is a necessary step before the implementation of clustering algorithms, due to the fact that most of them rely on the distances between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08508312-033e-4535-9713-d4edb5a177b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features =['Age', 'DaysSinceCreation', 'AverageLeadTime', 'LodgingRevenue','OtherRevenue', 'BookingsCanceled', 'BookingsNoShowed','BookingsCheckedIn', 'PersonsNights', 'RoomNights','Total_Revenue', 'Service_share', 'Booking_Success_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d1a5f-3e09-4aff-9887-d7ce5ad4b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlations(data_scaled[metric_features], 'spearman')\n",
    "corr_plot(data_scaled[metric_features], 'spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9cdce-32f1-47c1-9f4c-ad65b7594456",
   "metadata": {},
   "outputs": [],
   "source": [
    "Geo_perspective = ['Age',\n",
    "                   'Total_Revenue',\n",
    "                   'Region_Portugal',\n",
    "                   'Region_FRA/GBR',\n",
    "                   'Region_Africa/Asia/Oceania',\n",
    "                   'Region_Northern Europe',\n",
    "                   'Region_Southern Europe',\n",
    "                   'Region_Western Europe',\n",
    "                   'Region_Germany',\n",
    "                   'Region_Eastern Europe',\n",
    "                   'Region_South/North America']\n",
    "\n",
    "Funnel_perspective= ['Age',\n",
    "                     'Total_Revenue',\n",
    "                     'DistributionChannel_Corporate',\n",
    "                     'DistributionChannel_Direct',\n",
    "                     'DistributionChannel_Travel Agent/Operator',\n",
    "                     'DistributionChannel_GDS Systems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85463a57-e305-4b80-ad60-5335fde04c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Funnel_data = data_scaled[Funnel_perspective]\n",
    "\n",
    "Funnel_categorical_features = ['DistributionChannel_Corporate',\n",
    "                               'DistributionChannel_Direct',\n",
    "                               'DistributionChannel_Travel Agent/Operator',\n",
    "                               'DistributionChannel_GDS Systems']\n",
    "\n",
    "Funnel_metric_features = ['Total_Revenue','Age']\n",
    "\n",
    "Geo_data = data_scaled[Geo_perspective]\n",
    "\n",
    "Geo_metric_features = ['Age',\n",
    "                   'Total_Revenue']\n",
    "\n",
    "Geo_categorical_features = ['Region_Portugal',\n",
    "                            'Region_France',\n",
    "                            'Region_Africa/Asia/Oceania',\n",
    "                            'Region_Northern Europe',\n",
    "                            'Region_Southern Europe',\n",
    "                            'Region_Western Europe',\n",
    "                            'Region_Germany',\n",
    "                            'Region_Eastern Europe',\n",
    "                            'Region_South/North America',\n",
    "                            'Region_GreatBritain',\n",
    "                            'Region_Asia/Oceania']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff43e57",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Creating different perspectives in order to improve the interpretability of the clustering solution.\n",
    "\n",
    "> 1 - Geo_perspective: Contains Age, Revenue and Geographiical information about the customer.\n",
    "\n",
    "> 2 - Funnel_perspective: Conains Age, Revenue and information about the funnel through which the customer was attained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cda03f",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\">\n",
    "\n",
    "## <font color='SeaGreen'> 4. Modelling </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e40b04",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1\">\n",
    "\n",
    "### 4.1 Funnel Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee25f24",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1.1\">\n",
    "\n",
    "#### 4.1.1 Analyzing data with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b040e11b",
   "metadata": {},
   "source": [
    "Implementing PCA to reduce the number of features, improve data visualization, reduce multicollinearity, and enhance the interpretability of clustering results. By using PCA, we can improve the performance of the clustering algorithm and gain a better understanding of the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e9e7e-1343-4643-a236-4af3870afc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensionality of data\n",
    "pca = PCA()\n",
    "pca_feat = pca.fit_transform(Funnel_data)\n",
    "pca_feat  # What is this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7ad24-48c3-403d-b98c-364db1df85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output PCA table\n",
    "pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb64805-e775-4855-9b1a-fb095348abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a233c",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Plotting a scree plot of the PCA in order to find the optimal amount of principal components to keep for the modeling. <p>\n",
    "\n",
    "- 2 principal components explain a acceptable amount of the variance in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccd85a-b746-4f9a-b833-e433ca3b5628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform PCA again with the number of principal components you want to retain\n",
    "pca = PCA(n_components=2)\n",
    "pca_feat = pca.fit_transform(Funnel_data)\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=Funnel_data.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f22d6c-3a14-4666-8130-cfdb3ce6b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "Funnel_data = pd.concat([Funnel_data, pca_df], axis=1)\n",
    "Funnel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b670d9b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1.2\">\n",
    "\n",
    "#### 4.1.2 Applying K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=RS)\n",
    "    kmeans.fit(pca_df)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "km = KMeans()\n",
    "visualizer = KElbowVisualizer(km, k=(1,20),locate_elbow='optimal', random_state=RS)\n",
    "visualizer.fit(pca_df)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0fcf39",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Using the elbow-plot to find the optimal number of clusters to use in K-Means clustering. We choose the optimal number of clusters (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072067af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=RS)\n",
    "y_kmeans_funnel = kmeans.fit_predict(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c96ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqByCluster = Funnel_data.groupby(y_kmeans_funnel).size()\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "g = sns.countplot(x=y_kmeans_funnel, palette=[\"#824D99\", \"#4E78C4\", \"#57A2AC\", \"#7EB875\", \"#B3AF38\", \"#A75051\", '#b27d58'])\n",
    "\n",
    "\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "for index,data in enumerate(freqByCluster):\n",
    "    plt.text(x=index-0.2 , y=data+50 , s=f\"{data}\" , fontdict=dict(fontsize=plots_barTexts_fontSize))\n",
    "sns.despine()\n",
    "plt.title(\"Cluster cardinality\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Frequency in cluster\")\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n",
    "print('Average frequency is', round((sum(freqByCluster)/6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c2f6c",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Cluster cardinality refers to the number of data points that belong to a particular cluster in a clustering solution. Cluster cardinality can also be used to evaluate the quality of the clustering solution. A good clustering solution should create clusters with similar sizes, where each cluster has a significant number of data points. <p>\n",
    "\n",
    "- In our case we are using a barplot to compare the different cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "allDistances = kmeans.fit_transform(Funnel_data)\n",
    "\n",
    "Funnel_data['distanceToCentroid'] = np.min(allDistances,axis=1)\n",
    "magnitude = Funnel_data['distanceToCentroid'].groupby(y_kmeans_funnel).sum()\n",
    "X = Funnel_data.drop(columns=['distanceToCentroid'])\n",
    "\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "g = sns.barplot(x=magnitude.index, y=magnitude.values, palette=[\"#824D99\", \"#4E78C4\", \"#57A2AC\", \"#7EB875\", \"#B3AF38\", \"#A75051\", '#b27d58'])\n",
    "\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "for index,data in enumerate(magnitude):\n",
    "    plt.text(x=index-0.2 , y=data+50 , s=f\"{data:,.0f}\" , fontdict=dict(fontsize=plots_barTexts_fontSize))\n",
    "sns.despine()\n",
    "plt.title(\"Cluster magnitude\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Sum of distances to centroid\")\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n",
    "print('Average magnitude is', round((sum(magnitude)/6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3fbd2",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Cluster magnitude refers to the total distance of the points of a cluster to its associated centroid in a clustering solution. Cluster cardinality can also be used to evaluate the quality of the clustering solution. A good clustering solution should create clusters with a low cluster magnitude in order for the points to be represented by the centroid accurately. <p>\n",
    "\n",
    "- In our case we are using a barplot to compare the different distances of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17827262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "slope, intercept = np.polyfit(freqByCluster, magnitude, 1)\n",
    "line = slope * freqByCluster + intercept\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = magnitude - line\n",
    "\n",
    "# Calculate the RSS\n",
    "rss = np.sum(residuals**2)\n",
    "\n",
    "# Plot the data and the regression line\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "g = sns.regplot(x=freqByCluster, y=magnitude, scatter=True, seed=123,truncate=False, ci=None)\n",
    "\n",
    "# Add the regression line to the plot\n",
    "plt.plot(freqByCluster, line, color='r')\n",
    "\n",
    "# Add the sum of distances to the plot\n",
    "plt.text(0.05, 0.9, f'Sum of distances: {rss:.2f}', transform=ax.transAxes)\n",
    "\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.xaxis.set_major_formatter(tick)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "sns.despine()\n",
    "plt.title(\"Cardinality vs Magnitude\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Cardinality\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2f441",
   "metadata": {},
   "source": [
    "#### __Notes:__\n",
    "\n",
    "- With increasing cardinality the magnitude increases aswell. Therefore it is important to consider the magnitude in relation to the increasing cardinality. In the optimal case, the increase in magnitude should be related to the increase in cardinality, since this means that the clusters have a similar distance to the centroids despite a larger number of points. <p>\n",
    "\n",
    "- In our case we are using a scatterplot to compare these metrics of the clusters. The identified clusters should be as close as possible to the intersection line of the diagram for them to have a similar magnitude/cardinality distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13952b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = InterclusterDistance(kmeans)\n",
    "visualizer.fit(Funnel_data)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a989a84",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Visualizing the clusters in a 2D map helps us understand the size aswell as their distinction. The goal is to achieve a intercluster distance that is as high as possible, in order to have clusters that are easliy distinctable between each other. However, the intercluster distances should be as small as possible so that the respective cluster centroids represent all points in the cluster as best as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Add the cluster assignments to the dataframe\n",
    "pca_df['cluster'] = y_kmeans_funnel\n",
    "Funnel_data['cluster'] = y_kmeans_funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14988b93-7210-4c8a-aa3d-b4bba3e68db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Analyze the results by examining the cluster means\n",
    "cluster_means = Funnel_data[Funnel_metric_features+['cluster']].groupby('cluster').mean()\n",
    "cluster_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120821a",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "> 1 - Comparing the means of the variables across clusters in order to identify the unique characteristics of each cluster.\n",
    "\n",
    "- Cluster 0: high revenue, low age <p>\n",
    "\n",
    "- Cluster 1: medium revenue, high age<p>\n",
    "\n",
    "- Cluster 2: low revenus, medium age<p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ec6f5-5cc9-401d-8faf-cb9a8e3b65de",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2\">\n",
    "\n",
    "### 4.2 Geo data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feceb65",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2.1\">\n",
    "\n",
    "#### 4.2.1 Analyzing data with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbb308-5369-4d0b-948e-8735bc554842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensionality of data\n",
    "pca = PCA()\n",
    "pca_feat = pca.fit_transform(Geo_data)\n",
    "pca_feat  # What is this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3c393-d606-46cb-8fdd-4207ba8131a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output PCA table\n",
    "pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7dd6c-cd16-4a04-a767-627080e710a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a257ed",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Plotting a scree plot of the PCA in order to find the optimal amount of principal components to keep for the modeling. <p>\n",
    "\n",
    "- 6 principal components explain a acceptable amount of the variance in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222aa3f3-3cd2-4c32-81e0-d17fce27718a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform PCA again with the number of principal components you want to retain\n",
    "pca = PCA(n_components=6)\n",
    "pca_feat = pca.fit_transform(Geo_data)\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=Geo_data.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a71b6-e964-49f9-a898-57b34cdaf215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "Geo_data = pd.concat([Geo_data, pca_df], axis=1)\n",
    "Geo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ee240-9333-427b-9448-c6fd2ec3e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _color_red_or_green(val):\n",
    "    if val < -0.45:\n",
    "        color = 'background-color: red'\n",
    "    elif val > 0.45:\n",
    "        color = 'background-color: green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0a977-6467-4330-95f0-8a884b4d58fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interpreting each Principal Component\n",
    "loadings = Geo_data[Geo_metric_features + pca_feat_names].corr().loc[Geo_metric_features, pca_feat_names]\n",
    "loadings.style.applymap(_color_red_or_green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683838a-d737-417f-a381-9633c1609843",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2.2\">\n",
    "\n",
    "#### 4.2.2 Applying K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d472b95-26b5-419f-85e2-dbeece06b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=RS)\n",
    "    kmeans.fit(pca_df)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "km = KMeans()\n",
    "visualizer = KElbowVisualizer(km, k=(1,20),locate_elbow='optimal', random_state=RS)\n",
    "visualizer.fit(pca_df)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1599f3f",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Using the elbow-plot to find the optimal number of clusters to use in K-Means clustering. We choose the optimal number of clusters (7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be3637-0606-4195-9383-a3be78e6beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7, random_state=RS)\n",
    "y_kmeans_Geo = kmeans.fit_predict(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe207714-cb9b-443a-b137-cc5e8d0980f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqByCluster = Geo_data.groupby(y_kmeans_Geo).size()\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "g = sns.countplot(x=y_kmeans_Geo, palette=[\"#824D99\", \"#4E78C4\", \"#57A2AC\", \"#7EB875\", \"#B3AF38\", \"#A75051\", '#b27d58'])\n",
    "\n",
    "\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "for index,data in enumerate(freqByCluster):\n",
    "    plt.text(x=index-0.2 , y=data+50 , s=f\"{data}\" , fontdict=dict(fontsize=plots_barTexts_fontSize))\n",
    "sns.despine()\n",
    "plt.title(\"Cluster cardinality\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Frequency in cluster\")\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n",
    "print('Average frequency is', round((sum(freqByCluster)/6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b23ace",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Cluster cardinality refers to the number of data points that belong to a particular cluster in a clustering solution. Cluster cardinality can also be used to evaluate the quality of the clustering solution. A good clustering solution should create clusters with similar sizes, where each cluster has a significant number of data points. <p>\n",
    "\n",
    "- In our case we are using a barplot to compare the different cluster sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24fa20-2228-4e39-abb0-ce2e4a718159",
   "metadata": {},
   "outputs": [],
   "source": [
    "allDistances = kmeans.fit_transform(Geo_data)\n",
    "\n",
    "Geo_data['distanceToCentroid'] = np.min(allDistances,axis=1)\n",
    "magnitude = Geo_data['distanceToCentroid'].groupby(y_kmeans_Geo).sum()\n",
    "X = Geo_data.drop(columns=['distanceToCentroid'])\n",
    "\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "g = sns.barplot(x=magnitude.index, y=magnitude.values, palette=[\"#824D99\", \"#4E78C4\", \"#57A2AC\", \"#7EB875\", \"#B3AF38\", \"#A75051\", '#b27d58'])\n",
    "\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "for index,data in enumerate(magnitude):\n",
    "    plt.text(x=index-0.2 , y=data+50 , s=f\"{data:,.0f}\" , fontdict=dict(fontsize=plots_barTexts_fontSize))\n",
    "sns.despine()\n",
    "plt.title(\"Cluster magnitude\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Sum of distances to centroid\")\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n",
    "print('Average magnitude is', round((sum(magnitude)/6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cbc46",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Cluster magnitude refers to the total distance of the points of a cluster to its associated centroid in a clustering solution. Cluster cardinality can also be used to evaluate the quality of the clustering solution. A good clustering solution should create clusters with a low cluster magnitude in order for the points to be represented by the centroid accurately. <p>\n",
    "\n",
    "- In our case we are using a barplot to compare the different distances of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916ef85-4732-498b-a4c1-4b3e6e35004c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "slope, intercept = np.polyfit(freqByCluster, magnitude, 1)\n",
    "line = slope * freqByCluster + intercept\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = magnitude - line\n",
    "\n",
    "# Calculate the RSS\n",
    "rss = np.sum(residuals**2)\n",
    "\n",
    "# Plot the data and the regression line\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "g = sns.regplot(x=freqByCluster, y=magnitude, scatter=True, seed=123,truncate=False, ci=None)\n",
    "\n",
    "# Add the regression line to the plot\n",
    "plt.plot(freqByCluster, line, color='r')\n",
    "\n",
    "# Add the sum of distances to the plot\n",
    "plt.text(0.05, 0.9, f'Sum of distances: {rss:.2f}', transform=ax.transAxes)\n",
    "\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.xaxis.set_major_formatter(tick)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "sns.despine()\n",
    "plt.title(\"Cardinality vs Magnitude\", fontsize=plots_Title_fontSize)\n",
    "plt.xlabel(\"Cardinality\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.rc('axes', labelsize=subPlots_label_fontSize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bace2c",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- With increasing cardinality the magnitude increases aswell. Therefore it is important to consider the magnitude in relation to the increasing cardinality. In the optimal case, the increase in magnitude should be related to the increase in cardinality, since this means that the clusters have a similar distance to the centroids despite a larger number of points. <p>\n",
    "\n",
    "- In our case we are using a scatterplot to compare these metrics of the clusters. The identified clusters should be as close as possible to the intersection line of the diagram for them to have a similar magnitude/cardinality distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a796ba2-562a-46ff-b438-b667dd0f85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = InterclusterDistance(kmeans)\n",
    "visualizer.fit(Geo_data)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d9152",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Visualizing the clusters in a 2D map helps us understand the size aswell as their distinction. The goal is to achieve a intercluster distance that is as high as possible, in order to have clusters that are easliy distinctable between each other. However, the intercluster distances should be as small as possible so that the respective cluster centroids represent all points in the cluster as best as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b2ac2-9bfb-4854-9a58-39fbd9b7abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Add the cluster assignments to the dataframe\n",
    "pca_df['cluster'] = y_kmeans_Geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1737cc-f82c-43e0-b1cf-a4f3ded0f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Geo_data['cluster'] = y_kmeans_Geo\n",
    "Geo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc8e8e-8d92-4011-8220-386537393eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Analyze the results by examining the cluster means\n",
    "cluster_means = Geo_data[Geo_metric_features+['cluster']].groupby('cluster').mean()\n",
    "cluster_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1d3da",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    " - Comparing the means of the variables across clusters in order to identify the unique characteristics of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87058d4b-0aa8-415f-b634-ecfd87cad21e",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\">\n",
    "\n",
    "### 5. Cluster Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb3ae8",
   "metadata": {},
   "source": [
    "Merging the perspectives in order to achieve a more complete representation of the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b06d3a-68d4-4e49-95eb-3c4826a6d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column called 'Geo_Clus' \n",
    "data_scaled['Geo_Clus'] = y_kmeans_Geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f58f9-6184-4cf5-b735-cecff694a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column called 'Funnel_Clus' \n",
    "data_scaled['Funnel_Clus'] = y_kmeans_funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49cf37-40b7-4ff8-a706-d55335145b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all the existing columns \n",
    "data_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0bc99-a487-4da5-9e47-278e41fe77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the features for the merge\n",
    "merging_features = ['Total_Revenue','Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2862c16-663f-475a-b30f-8977c95a0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of the concatenated cluster labels\n",
    "df_centroids = data_scaled.groupby(['Geo_Clus','Funnel_Clus'])\\\n",
    "    [merging_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0740da8-d940-43a6-8084-188b3726387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hierarchical clustering to merge the concatenated cluster centroids\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None)\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f37191-6663-4e81-9707-c5631d91c671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 5.5\n",
    "dendrogram(linkage_matrix, truncate_mode='level', labels=df_centroids.index, p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "#plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering Dendrogram', fontsize=20)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "# Save the plot as an PNG file\n",
    "plt.savefig('hc.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632b7fc",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "    \n",
    "- Using the dendogram of the hierarchical clustering to identify a optimal amount of clusters by looking for the point on the dendrogram where the distance between clusters starts to increase rapidly. <p>\n",
    "\n",
    "- 5 clusters seem to be optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cea635-9f86-4ebe-8cc9-b15ea7319570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    n_clusters=5\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ecd26",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- The hierarchical clustering assignes the merged clusters to a new cluster.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48b5cd-df76-4721-8e92-de1f72d038ac",
   "metadata": {},
   "source": [
    "#### Merged Clusters Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34f550-8396-47b1-a238-0cde8d018c11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "\n",
    "df_ = data_scaled.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['Geo_Clus'],row['Funnel_Clus'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Merged cluster centroids\n",
    "df_.groupby('merged_labels').mean()[merging_features+['Booking_Success_Rate','AverageLeadTime']].style.highlight_max(color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185217b5",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    " 1 - Adding the variables Booking_Success_Rate and AverageLeadTime to increase the information gain.\n",
    "\n",
    " 2 - Comparing the means of the new merged clusters has interesting insights: \n",
    "\n",
    "  - Cluster 0: Highest AverageLeadTime <p>\n",
    "  \n",
    "  - Cluster 2: Booking_Succsess_Rate<p>\n",
    "  \n",
    "  - Cluster 3: Highest Total_Revenue and Age<p>\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e97db-66cf-4654-b7b0-56c4001bb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled['merged_labels'] = df_['merged_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e96ba6-c48e-44a3-98e7-5eb3f4c6bb01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a list containing the different regions \n",
    "Y = ['Region_Portugal',\n",
    "     'Region_FRA/GBR',\n",
    "     'Region_Africa/Asia/Oceania',\n",
    "     'Region_Northern Europe',\n",
    "     'Region_Southern Europe',\n",
    "     'Region_Western Europe',\n",
    "     'Region_Germany',\n",
    "     'Region_Eastern Europe',\n",
    "     'Region_South/North America']\n",
    "\n",
    "X = 'merged_labels'\n",
    "\n",
    "# Count frequency of X in Y\n",
    "df_freq = pd.DataFrame(index=Y, columns=['Cluster_{}'.format(i) for i in range(data_scaled[X].nunique())])\n",
    "\n",
    "for y in Y:\n",
    "    for i in range(data_scaled[X].nunique()):\n",
    "        df_freq.loc[y, 'Cluster_{}'.format(i)] = sum((data_scaled[y] == 1) & (data_scaled[X] == i))\n",
    "\n",
    "df_freq = df_freq.astype(int)\n",
    "df_freq.style.highlight_max(color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675d14d",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "1 - Comparing the amount of customers from different Regions in each of the clusters.\n",
    "\n",
    "- Cluster 0: Most customers from France and Great Britain\n",
    "- Cluster 1: Most customers from Western Europe\n",
    "- Cluster 2: Most customers from Portugal\n",
    "- Cluster 3: Most customers from America\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f6449-ebb1-4028-95ba-6f59583a3754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a list containing the different distribution channels \n",
    "Y = ['DistributionChannel_Corporate',\n",
    "     'DistributionChannel_Direct',\n",
    "     'DistributionChannel_Travel Agent/Operator',\n",
    "     'DistributionChannel_GDS Systems']\n",
    "\n",
    "X = 'merged_labels'\n",
    "\n",
    "# Count frequency of X in Y\n",
    "df_freq2 = pd.DataFrame(index=Y, columns=['Cluster_{}'.format(i) for i in range(data_scaled[X].nunique())])\n",
    "\n",
    "for y in Y:\n",
    "    for i in range(data_scaled[X].nunique()):\n",
    "        df_freq2.loc[y, 'Cluster_{}'.format(i)] = sum((data_scaled[y] == 1) & (data_scaled[X] == i))\n",
    "\n",
    "df_freq2 = df_freq2.astype(int)\n",
    "# Highlight max values in each column with dark blue\n",
    "df_freq2.style.highlight_max(color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81fea4",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "1 - Comparing the amount of customers in each cluster that got aquired by which distribution channel.\n",
    "\n",
    "- Travel Agent: Most of the customers in clusters 0,1 and 2 got aqquired trough a Travel Agent.\n",
    "\n",
    "- Direct: Most of the customers in cluster 3 got aqquired directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095976e-b65e-4132-9418-9cc5694db2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a list containing the different market segments \n",
    "Y = ['MarketSegment_Corporate',\n",
    "     'MarketSegment_Direct',\n",
    "     'MarketSegment_Travel Agent/Operator',\n",
    "     'MarketSegment_Groups',\n",
    "     'MarketSegment_Other',\n",
    "     'MarketSegment_Complementary',\n",
    "     'MarketSegment_Aviation']\n",
    "\n",
    "X = 'merged_labels'\n",
    "\n",
    "# Count frequency of X in Y\n",
    "df_freq3 = pd.DataFrame(index=Y, columns=['Cluster_{}'.format(i) for i in range(data_scaled[X].nunique())])\n",
    "\n",
    "for y in Y:\n",
    "    for i in range(data_scaled[X].nunique()):\n",
    "        df_freq3.loc[y, 'Cluster_{}'.format(i)] = sum((data_scaled[y] == 1) & (data_scaled[X] == i))\n",
    "\n",
    "df_freq3 = df_freq3.astype(int)\n",
    "# Highlight max values in each column with dark blue\n",
    "df_freq3.style.highlight_max(color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47396354",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Clusters 0,1 and 2 have the highest share of customers in the Marketsegment others.\n",
    "\n",
    "- Cluser 3 has the highest share in the Segment direct.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9a33f-016e-4aed-8b15-b703ef803fb2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"6.\">\n",
    "\n",
    "## <font color='SeaGreen'> 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ebf5d-c1f7-423d-9c1e-e57a693b7d70",
   "metadata": {},
   "source": [
    "<font color='SeaGreen'> __Evaluate the model's performance and compare it to the project's success criteria. If the model does not meet the criteria, refine the model and repeat the process.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the entire dataset\n",
    "data_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c3c73-e1b5-44bb-8467-5dcb7a9f8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label impoartant features\n",
    "important = ['Age', 'DaysSinceCreation', 'AverageLeadTime', 'LodgingRevenue',\n",
    "       'OtherRevenue', 'BookingsCanceled', 'BookingsNoShowed',\n",
    "       'BookingsCheckedIn', 'PersonsNights', 'RoomNights',\n",
    "       'DistributionChannel_Corporate', 'DistributionChannel_Direct',\n",
    "       'DistributionChannel_Travel Agent/Operator',\n",
    "       'DistributionChannel_GDS Systems','Total_Revenue', 'Service_share', 'Booking_Success_Rate',\n",
    "       'Preference_Count', 'Region_Portugal', 'Region_FRA/GBR',\n",
    "       'Region_Africa/Asia/Oceania', 'Region_Northern Europe',\n",
    "       'Region_Southern Europe', 'Region_Western Europe', 'Region_Germany',\n",
    "       'Region_Eastern Europe', 'Region_South/North America']\n",
    "#Preparing the data\n",
    "X = data_scaled[important]\n",
    "y = data_scaled['merged_labels']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fitting the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"It is estimated that in average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb6545",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Using a decision tree to see how accurately our clustering can label new customers. 70.06% are predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741977b3-9eb2-4033-94ab-3e0fcdd386ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4186d",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- The values in the features Region_FRA/GBR,                    Geo_Clus, Region_Portugal and DistributionChannel_Travel Agent/Operator have the highest impact on the cluster the customer will be assinged to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f71af-d236-4549-a300-2c8c6d466594",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = ['0','1','2','3', '4']#, '5', '6']\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1 ,ncols = 1,figsize = (10,10), dpi=300)\n",
    "\n",
    "tree.plot_tree(dt, \n",
    "                   feature_names=important,  \n",
    "                   class_names=cn,\n",
    "                   filled=True)\n",
    "\n",
    "#fig.savefig('tree.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9be9b",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "- Visualizing the decision tree to increase the understandability of the cluster assignment with the 4 features of the highest feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43441d-920e-44ef-be94-437f94a372b3",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"7\">\n",
    "\n",
    "## <font color='SeaGreen'> 7. Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744526b-6bc5-49c3-997c-30c7ff24538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining columns to the dataframe where the data is not normalized\n",
    "data_clus_pro = data_beforeNorm.join(data_scaled[['Geo_Clus', 'Funnel_Clus', 'merged_labels']])\n",
    "data_clus_pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934412b-0117-456e-bcd1-3e4f0dc2775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of all the metric features\n",
    "metric_features = ['Age',\n",
    "                   'DaysSinceCreation',\n",
    "                   'AverageLeadTime',\n",
    "                   'LodgingRevenue',\n",
    "                   'PersonsNights',\n",
    "                   'RoomNights',\n",
    "                   'Total_Revenue',\n",
    "                   'Service_share',\n",
    "                   'Booking_Success_Rate',\n",
    "                   'Preference_Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a95b9e-4138-4f11-af67-5af6edadae35",
   "metadata": {},
   "source": [
    "#### Geographic Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f15b-5627-4a8b-bcf9-d45bdad4374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the cluster profiles of the value, consumption and merged perspectives\n",
    "cluster_profiles(\n",
    "    data_scaled[metric_features+['Geo_Clus']], \n",
    "    label_columns = ['Geo_Clus'], \n",
    "    figsize = (45, 20), \n",
    "    compar_titles = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172867ab",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "\n",
    "- Visualizing the cluster means for the metric features based on the geographic profile, aswell as the amount of customers in each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8a948-aa52-48ff-b92f-2ef9718c376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the mean value of each variable of each cluster\n",
    "grouped_geo = data_clus_pro[metric_features+['Geo_Clus']].groupby('Geo_Clus').mean().reset_index(drop=True).T\n",
    "grouped_geo.style.highlight_max(axis=1,color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db508617",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "\n",
    "Comparing means of the clusters:\n",
    "  \n",
    "- Cluster 0: Highest PersonsNights value\n",
    "- Cluster 2: Highest SaysSinceCreation and Booking_Success_Rate\n",
    "- Cluster 3: Highest Preference_Count\n",
    "- Cluster 4: Highest LodgingRevenue, RoomNights and Total_Revenue\n",
    "- Cluster 6: Highest Age, AverageLeadTime, Service_share\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38e534-411e-4065-8ac9-459b330975de",
   "metadata": {},
   "source": [
    "#### Funnel Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe26ed-b401-4d0b-b58e-56aedf4d8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the cluster profiles of the value, consumption and merged perspectives\n",
    "cluster_profiles(\n",
    "    data_clus_pro[metric_features+['Funnel_Clus']], \n",
    "    label_columns = ['Funnel_Clus'], \n",
    "    figsize = (45, 20), \n",
    "    compar_titles = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08008d7b",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "\n",
    "- Visualizing the cluster means for the metric features based on the funnel profile, aswell as the amount of customers in each cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6981b2-e8c2-4a2d-b64c-e21adf0c35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the mean value of each variable of each cluster\n",
    "grouped_funnel = data_clus_pro[metric_features+['Funnel_Clus']].groupby('Funnel_Clus').mean().reset_index(drop=True).T\n",
    "grouped_funnel.style.highlight_max(axis=1,color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17456c-55c9-4b9a-a57c-7ccb7668ced3",
   "metadata": {},
   "source": [
    "#### Final Merged Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b01bb-f641-466e-b2c1-8b2a0d24db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the cluster profiles of the value, consumption and merged perspectives\n",
    "cluster_profiles(\n",
    "    data_scaled[metric_features+['merged_labels']], \n",
    "    label_columns = ['merged_labels'], \n",
    "    figsize = (45, 20), \n",
    "    compar_titles = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ac905",
   "metadata": {},
   "source": [
    "__Notes:__ \n",
    "\n",
    "- Visualizing the merged cluster means for the metric features, aswell as the amount of customers in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbbabd-e66b-4091-89da-2c5e3e23a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the mean value of each variable of each cluster\n",
    "grouped_merged = data_clus_pro[metric_features+['merged_labels']].groupby('merged_labels').mean().reset_index(drop=True).T\n",
    "grouped_merged.style.highlight_max(axis=1,color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69166ba-24eb-4017-939c-0344a40ea610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe from Modelling/MergingClusters/Merged Clusters Centroids\n",
    "df_freq.style.highlight_max(color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdf619-dc40-40f1-aac9-a867229ab356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe from Modelling/MergingClusters/Merged Clusters Centroids\n",
    "df_freq2.style.highlight_max(axis=1, color=\"#4E78C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f24d9-6696-400e-99c4-de4eaf9edb20",
   "metadata": {},
   "source": [
    "Cluster 0\n",
    "\n",
    "- Lowest Revenue generated out of all others.\n",
    "- Mainly Portuguese clients, who are not present on other clusters.\n",
    "- Segment that books the closest to arrival.\n",
    "- The lowest amount of People per Night, same for Rooms per Night, although the difference is more marginal.\n",
    "- Considerable spending on Hotel Services.\n",
    "- Corporate Funnel is the most appropriate funnel for targeting.\n",
    "- Most attendance post booking.\n",
    "- Clients in this segment have a very low special request count.\n",
    "\n",
    "Cluster 1\n",
    "\n",
    "- Considerably high Revenue generated.\n",
    "- Mainly French, British, and Southern Europe clients.\n",
    "- The highest amount of People per Night, 2nd highest Rooms per Night.\n",
    "- Considerable spending on Hotel Services.\n",
    "- High attendance post booking.\n",
    "- Significantly high special request count.\n",
    "- Travel agencies and other operators are the most appropriate funnel for targeting.\n",
    "- Direct funnelling, could be explored, due to considerable frequency with a discount for booking on arrival, keeping higher margins.\n",
    "\n",
    "Cluster 2\n",
    "\n",
    "- Oldest segment out of all.\n",
    "- Segment that books the farthest to arrival.\n",
    "- Considerably Low Revenue generated.\n",
    "- German clients, that are not present in other segments, and North and South American.\n",
    "- Highest spend on Services, these customers should be targeted with tours, restaurant deals, massages, SPAs, and all existing service ranges available.\n",
    "- Highest preference count.\n",
    "- Travel Agencies should be the only ones considered for marketing campaigns.\n",
    "\n",
    "Cluster 3\n",
    "\n",
    "- Highest spend on rooms, and lowest use of services, compared to room spend.\n",
    "- Most Valuable cluster in Total revenue Count.\n",
    "- Exclusively North American Clients.\n",
    "- Use of booking on Arrival with the highest frequency.\n",
    "- Marketing campaigns should be prioritised towards this cluster.\n",
    "\n",
    "Cluster 4\n",
    "\n",
    "- Significantly high Revenue, Persons, and Rooms Booked per Night.\n",
    "- Low demand for Hotel Services.\n",
    "- Significantly high special request count.\n",
    "- Segment with low regional differentiation, englobing Asian, African, Oceanic, and European clients.\n",
    "- Highest use of booking on Arrival, and a significantly high count of Travel Agency Bookings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697eba0d-e096-4506-a0f9-3ae078d8f374",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60955e4-5ced-408f-9863-03e5ac40335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "two_dim = TSNE(random_state=42).fit_transform(data_scaled[important])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1314272-760b-4cf8-ab4f-98f83ffa7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=data_scaled['merged_labels'], colormap='inferno', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f568f52-bfe2-4320-9120-ef1e7b3ce547",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e3d7f-9d60-4b1a-b45c-f1f4a7574b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform UMAP on the scaled data\n",
    "umap_reducer = umap.UMAP(n_components=2)\n",
    "umap_data = umap_reducer.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09802dc8-cd27-4f4c-b507-9801bda28463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the UMAP results\n",
    "plt.scatter(umap_data[:,0], umap_data[:,1], c=data_scaled['merged_labels'])\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.title('2D UMAP of Scaled Data')\n",
    "# Save the plot as an PNG file\n",
    "plt.savefig('umap_plot.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
